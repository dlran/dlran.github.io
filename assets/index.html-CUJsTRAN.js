import{_ as a,c as s,a as i,o as h}from"./app-CfvC7fsZ.js";const l={};function n(t,e){return h(),s("div",null,e[0]||(e[0]=[i(`<p>本文将介绍如何使用 <a href="https://ollama.com" target="_blank" rel="noopener noreferrer">Ollama</a> 本地部署 DeepSeek 团队发布的 <strong>DeepSeek-R1</strong> 系列大模型，并结合开源前端UI，实现大模型token自由。</p><h2 id="一、准备工作-安装-ollama-后端" tabindex="-1"><a class="header-anchor" href="#一、准备工作-安装-ollama-后端"><span>一、准备工作：安装 Ollama 后端</span></a></h2><h3 id="什么是-ollama" tabindex="-1"><a class="header-anchor" href="#什么是-ollama"><span>什么是 Ollama？</span></a></h3><p>Ollama 是 Meta 开源的本地大型语言模型(LLM) 运行框架，允许用户在自己的设备上轻松地部署和运行各种开源LLM（如 LLaMA、Mistral、DeepSeek-R1 等）。</p><h3 id="安装方式" tabindex="-1"><a class="header-anchor" href="#安装方式"><span>安装方式</span></a></h3><p>打开 <a href="https://ollama.com" target="_blank" rel="noopener noreferrer">Ollama 官网</a>，根据你的操作系统下载对应版本并安装。</p><p>安装完成后，在终端执行如下命令测试是否成功：</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">ollama</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>如果能够看到 Ollama 的命令行帮助信息，说明安装成功。</p><hr><h2 id="二、拉取并运行-deepseek-r1-模型" tabindex="-1"><a class="header-anchor" href="#二、拉取并运行-deepseek-r1-模型"><span>二、拉取并运行 DeepSeek-R1 模型</span></a></h2><p>DeepSeek 团队推出的 R1 系列模型（如 <code>deepseek-r1:7b</code>, <code>deepseek-r1:32b</code>）在性能上表现出色，同时开源可商用，非常适合本地部署实验使用。</p><h3 id="拉取模型" tabindex="-1"><a class="header-anchor" href="#拉取模型"><span>拉取模型</span></a></h3><p>Ollama 提供了与 Docker 类似的命令结构。使用以下命令拉取并运行 DeepSeek-R1 的 7B 版本：</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">ollama</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> run</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> deepseek-r1:7b</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>这条命令将自动下载模型并启动一个本地推理实例。</p><p>如果你有足够的 GPU 显存（建议至少 48GB），也可以尝试更大版本：</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">ollama</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> run</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> deepseek-r1:32b</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>Ollama 会根据你的硬件自动使用 GPU 或回退到 CPU（如果 GPU 不支持）。</p><hr><h2 id="三、前端界面选择-web-ui-与-chrome-插件" tabindex="-1"><a class="header-anchor" href="#三、前端界面选择-web-ui-与-chrome-插件"><span>三、前端界面选择：Web UI 与 Chrome 插件</span></a></h2><p>虽然 Ollama 支持命令行对话，但拥有一个可视化前端能带来更高效的交互体验。你可以根据自己的使用习惯选择以下两种方式之一：</p><h3 id="_1-使用-open-webui-推荐" tabindex="-1"><a class="header-anchor" href="#_1-使用-open-webui-推荐"><span>1. 使用 Open WebUI（推荐）</span></a></h3><p><a href="https://github.com/open-webui/open-webui" target="_blank" rel="noopener noreferrer">Open WebUI</a> 对接本地Ollama后端，同时也兼容 OpenAI 的API风格，能调用远程模型服务（如 GPT-4）</p><h4 id="部署步骤-docker-方式" tabindex="-1"><a class="header-anchor" href="#部署步骤-docker-方式"><span>部署步骤（Docker 方式）：</span></a></h4><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">docker</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> run</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> -d</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> --name</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> open-webui</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> -p</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> 3000:3000</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> \\</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">  -e</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> OLLAMA_BASE_URL=http://host.docker.internal:11434</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> \\</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">  --restart</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> unless-stopped</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> \\</span></span>
<span class="line"><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">  ghcr.io/open-webui/open-webui:main</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><blockquote><p>注意：Ollama 默认监听在 <code>http://localhost:11434</code>，Docker 中访问主机地址需使用 <code>host.docker.internal</code>（适用于 macOS 与 Windows）。</p></blockquote><p>完成后访问：<a href="http://localhost:3000" target="_blank" rel="noopener noreferrer">http://localhost:3000</a> 即可看到前端界面，选择模型后即可开始对话。</p><h3 id="_2-使用-chrome-插件-page-assist" tabindex="-1"><a class="header-anchor" href="#_2-使用-chrome-插件-page-assist"><span>2. 使用 Chrome 插件 Page Assist</span></a></h3><p><a href="https://chromewebstore.google.com/detail/jfgfiigpkhlkbnfnbobbkinehhfdhndo?utm_source=item-share-cb" target="_blank" rel="noopener noreferrer">Page Assist</a> 是一款 Chrome 插件，可以直接接入本地 Ollama 模型。</p><hr><h2 id="四、总结与建议" tabindex="-1"><a class="header-anchor" href="#四、总结与建议"><span>四、总结与建议</span></a></h2><p>本地部署 DeepSeek-R1 模型非常简单，得益于 Ollama 的抽象封装和良好的模型生态：</p><ul><li>如果你是开发者，命令行使用足以满足调试需求；</li><li>如果你注重交互体验，推荐使用 Open WebUI；</li><li>如果你希望在浏览器中使用 AI 辅助，Page Assist 插件是一个不错的选择。</li></ul><h3 id="推荐配置参考" tabindex="-1"><a class="header-anchor" href="#推荐配置参考"><span>推荐配置参考</span></a></h3><table><thead><tr><th>模型版本</th><th>推荐显存</th><th>说明</th></tr></thead><tbody><tr><td>deepseek-r1:7b</td><td>≥ 16GB</td><td>日常对话、高性能本地部署</td></tr><tr><td>deepseek-r1:32b</td><td>≥ 48GB（或更多）</td><td>追求极致性能、服务器部署</td></tr></tbody></table><hr><h2 id="五、后续可探索内容" tabindex="-1"><a class="header-anchor" href="#五、后续可探索内容"><span>五、后续可探索内容</span></a></h2><ul><li>Open WebUI 接入 Google Gemini 的 API（免费）；</li><li>AnythingLLM嵌入私有知识库</li><li>推理优化：如 <code>llm.cpp</code>、<code>GGUF</code> 等模型格式的轻量化加载。</li></ul><hr>`,40)]))}const d=a(l,[["render",n]]),p=JSON.parse('{"path":"/article/3C5LztjJ/","title":"本地部署 DeepSeek-R1 大模型","lang":"zh-CN","frontmatter":{"title":"本地部署 DeepSeek-R1 大模型","tags":["AI"],"createTime":"2025/01/12 22:45:57","permalink":"/article/3C5LztjJ/"},"readingTime":{"minutes":2.63,"words":788},"git":{},"filePathRelative":"Operations/3C5LztjJ.md","headers":[],"categoryList":[{"id":"456d0d","sort":10000,"name":"Operations"}]}');export{d as comp,p as data};
